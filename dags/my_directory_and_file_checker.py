from pathlib import Path
from airflow import DAG
from airflow.operators.python import BranchPythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import date, datetime, timedelta
import os
import mysql
import pandas as pd
from datetime import datetime
from airflow.providers.mysql.hooks.mysql import MySqlHook

from sqlalchemy import create_engine, text

import logging
import shutil



# Configuration du logging
LOG_FILE = "errors.log"
logging.basicConfig(filename=LOG_FILE, level=logging.ERROR, format="%(asctime)s - %(levelname)s - %(message)s")


# Colonnes attendues et leurs types
EXPECTED_COLUMNS = [
    'accounting_date', 'lot_number', 'type_ecriture',
    'type_document','document_number','article_number',
    'variant_code','description','package_number',
    'store_code','unit_code','created_by',
    'quantity','quantity_in_sac','quantity_invoiced',
    'remaining_quantity','quantity_reserved','lettering_writing',
    'sales_amount_actual','total_cost_actual','total_cost_not_included',
    'is_open','order_type','created_at',
    "sequence_number",'kor_by_reception','kor_input'
    ]
        
COLUMN_TYPES = {
    EXPECTED_COLUMNS[0]: date,EXPECTED_COLUMNS[1]: str, EXPECTED_COLUMNS[2]: str,
    EXPECTED_COLUMNS[3]: str,EXPECTED_COLUMNS[4]: str, EXPECTED_COLUMNS[5]: str,
    EXPECTED_COLUMNS[6]: str,EXPECTED_COLUMNS[7]: str,EXPECTED_COLUMNS[8]: str,
    EXPECTED_COLUMNS[9]: str,EXPECTED_COLUMNS[10]: str,EXPECTED_COLUMNS[11]: str,
    EXPECTED_COLUMNS[12]: float,EXPECTED_COLUMNS[13]: float,EXPECTED_COLUMNS[14]: float,
    EXPECTED_COLUMNS[15]: str,EXPECTED_COLUMNS[16]: float,EXPECTED_COLUMNS[17]: float,
    EXPECTED_COLUMNS[18]: float,EXPECTED_COLUMNS[19]: float,EXPECTED_COLUMNS[20]: float,
    EXPECTED_COLUMNS[21]: str,EXPECTED_COLUMNS[22]: str,EXPECTED_COLUMNS[23]: datetime,
    EXPECTED_COLUMNS[24]: int,EXPECTED_COLUMNS[25]: float,EXPECTED_COLUMNS[26]: float,
}

RENAMED_COLUMNS={
    "Date comptabilisation": EXPECTED_COLUMNS[0],"N¬∞ lot": EXPECTED_COLUMNS[1],"Type √©criture":EXPECTED_COLUMNS[2],
    "Type document":EXPECTED_COLUMNS[3],"N¬∞ document":EXPECTED_COLUMNS[4],"N¬∞ article":EXPECTED_COLUMNS[5],
    "Code variante":EXPECTED_COLUMNS[6],"Description":EXPECTED_COLUMNS[7],"N¬∞ Package":EXPECTED_COLUMNS[8],
    "Code magasin":EXPECTED_COLUMNS[9],"Code unit√©":EXPECTED_COLUMNS[10],"Cr√©√© par":EXPECTED_COLUMNS[11],
    "Quantit√©":EXPECTED_COLUMNS[12],"Quantite en sac":EXPECTED_COLUMNS[13],"Quantit√© factur√©e":EXPECTED_COLUMNS[14],
    "Quantit√© restante":EXPECTED_COLUMNS[15],"Quantit√© r√©serv√©e":EXPECTED_COLUMNS[16],"Ecriture lettrage":EXPECTED_COLUMNS[17],
    "Montant vente (r√©el)":EXPECTED_COLUMNS[18],"Co√ªt total (r√©el)":EXPECTED_COLUMNS[19],"Co√ªt total (non incorp.)":EXPECTED_COLUMNS[20],
    "Ouvert":EXPECTED_COLUMNS[21],"Type de commande":EXPECTED_COLUMNS[22],"Cr√©√© √†":EXPECTED_COLUMNS[23],
    "N¬∞ s√©quence":EXPECTED_COLUMNS[24],"KOR par R√©ception":EXPECTED_COLUMNS[25],"KOR INPUT":EXPECTED_COLUMNS[26]

}

# D√©finition des variables
#DIRECTORY_PATH = "/opt/airflow/dags/shares"  # Remplacez par votre chemin
DIRECTORY_PATH = "/opt/airflow/files"  # Remplacez par votre chemin
ALLOWED_TYPES = [".csv"]  # Types de fichiers accept√©s
ENCODINGS = ["utf-8", "ISO-8859-1", "Windows-1252"]  # Liste des encodages possibles

LOG_DIR="logs"
ERROR_FILENAME="error_date_execution.txt"
SUCCESS_FILENAME="success_date_execution.txt"

IN_DIR="in"
OUT_DIR="out"
#DATABASE_URL = "mysql+pymysql://root:root225@host.docker.internal:3306/testimportdb"

os.makedirs(DIRECTORY_PATH, exist_ok=True)


from airflow.providers.mysql.hooks.mysql import MySqlHook


def import_temp_ecc_to_ecc():
    try:
        # Initialiser le hook MySQL
        mysql_hook = MySqlHook(mysql_conn_id='mysql_conn')
        select_query = """ SELECT * FROM temp_ecc; """

        records = mysql_hook.get_records(select_query)
          # Pr√©parer les donn√©es pour l'insertion ou la mise √† jour
        data_to_insert = [
            (
                f"{record[3]}{record[4]}",  # Concat√©ner `N¬∞ sequence` et `N¬∞document` pour cr√©er `id`
                f"{record[0].strftime('%Y-%m-%d')}", f"{record[1]}",f"{record[2]}",
                f"{record[3]}",f"{record[4]}",f"{record[5]}",f"{record[6]}",
                f"{record[7]}",f"{record[8]}",f"{record[9]}",f"{record[10]}",
                f"{record[11]}",f"{record[12]}",f"{record[13]}",f"{record[14]}",
                f"{record[15]}",f"{record[16]}",f"{record[17]}",f"{record[18]}",
                f"{record[19]}",f"{record[20]}",f"{record[21]}",f"{record[22]}",
                f"{record[23].strftime('%Y-%m-%d %H:%M')}",f"{record[24]}",f"{record[25]}",f"{record[26]}",
            )
            for record in records
        ]


        delete_query = """
            DELETE FROM ecc WHERE id IN (%s)
        """ % ", ".join(["%s"] * len(data_to_insert))

        mysql_hook.run(delete_query, parameters=[row[0] for row in data_to_insert])  # Supprime les anciens enregistrements

        # Ins√®re les nouvelles donn√©es
        mysql_hook.insert_rows(
            table="ecc",
            rows=data_to_insert,
            target_fields=["id", "accounting_date", "lot_number", "type_ecriture", "document_number", "sequence_number"]
        )
        return 'end'
    except Exception as e:
        print(f"‚ùå Erreur lors de l'importation des donn√©es : {e}")
        



def move_file_to_out(file_path, out_directory,statusMove:bool=True):
    """D√©place le fichier trait√© vers le dossier OUT en ajoutant la date et l'heure au nom du fichier."""
    try:
        # V√©rifier si le dossier OUT existe, sinon le cr√©er
        if not os.path.exists(out_directory):
            os.makedirs(out_directory)

        # R√©cup√©rer le nom du fichier et son extension
        file_name, file_extension = os.path.splitext(os.path.basename(file_path))

        # Obtenir la date et l'heure actuelles au format souhait√©
        current_datetime = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Cr√©er le nouveau nom de fichier en y ajoutant la date et l'heure
        file_name = f"{file_name}_{current_datetime}{file_extension}"
        if statusMove:
            # D√©finir le chemin de destination
            destination_path = os.path.join(f"{out_directory}/success", file_name)
        else:
            destination_path = os.path.join(f"{out_directory}/error", file_name)


        # D√©placer le fichier
        shutil.move(file_path, destination_path)

        # Message de succ√®s
        suc_message = f"‚úÖ Fichier d√©plac√© vers {destination_path}"
        print(suc_message)
        # Appeler la fonction log_message si elle est d√©finie
        log_message(LOG_DIR,SUCCESS_FILENAME, suc_message)

    except Exception as e:
        # G√©rer les exceptions et afficher un message d'erreur
        err_message = f"‚ùå Erreur lors du d√©placement du fichier : {e}"
        print(err_message)
        log_message(LOG_DIR,ERROR_FILENAME, err_message)



def renamed_panda_colonnes(pandasFile: pd.DataFrame,expected_columns:any,renamed_columns:object) -> pd.DataFrame:
    if not isinstance(pandasFile, pd.DataFrame):
        print("‚ùå Erreur : L'entr√©e doit √™tre un DataFrame pandas.")
        return None
    print(f"expected_columns20202 : {expected_columns}")
    print(f"renamed_columns2022 : {renamed_columns}")
    pandasFile.rename(columns=renamed_columns, inplace=True)
    missing_columns = set(expected_columns) - set(pandasFile.columns)
    if missing_columns:
        print(f"‚ùå Colonnes manquantes : {', '.join(missing_columns)}")
        print(f"‚ùå BORISColonnes trouvees : {', '.join(pandasFile.columns)}")
        return None
    return pandasFile

def log_message(fpath,filename, message):
    """√âcrit un message dans un fichier log."""
    file_path = os.path.join(f'{DIRECTORY_PATH}/{fpath}', filename)  # Chemin du fichier dans logs/
    with open(file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")


# Fonction pour v√©rifier l'existence du r√©pertoire
def check_directory(directory_path):
    if os.path.isdir(directory_path):
        return "check_file_in_directory"
    else:
        return "end"

# Fonction pour v√©rifier l'existence d'un type de fichier sp√©cifique
def check_file_in_directory(directory_path,allow_types):
    print('ca marche',directory_path,allow_types)
    files = [f for f in os.listdir(directory_path) if os.path.splitext(f)[1] in allow_types]
    if files:
        return "verify_file_reliability"
    else:
        log_message(LOG_DIR,ERROR_FILENAME, 'fichier csv non trouv√©')
        return "end"

def read_file(file,file_path,encodings,expected_columns,renamed_columns)->pd:
    result_read_file:pd=None
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding, delimiter=";")
            # üîπ Supprimer les lignes totalement vides
            df.replace("", pd.NA).dropna(how="all")
            print(f"\nüìÇ Contenu du fichier ({encoding}): {file}")
            print(df.head())  # Afficher les premi√®res lignes
            result_read_file=df
            break  # Sortir de la boucle si la lecture r√©ussit
        except UnicodeDecodeError as e:
            err_message=f"‚ùå Erreur d'encodage ({encoding}) pour {file}: {e}"
            logging.error(err_message)
            log_message(LOG_DIR,ERROR_FILENAME, err_message)
        except Exception as e:
            err_message=f"‚ö†Ô∏è Erreur lors de la lecture de {file} avec {encoding}: {e}"
            logging.error(err_message)
            log_message(LOG_DIR,ERROR_FILENAME, err_message)
            break  # Ne pas tester d'autres encodages si une autre erreur survient
    if result_read_file is not None:
        rn=renamed_panda_colonnes(result_read_file,expected_columns,renamed_columns)  # Renommer les colonnes si possible
        return rn 
    else:
        return None  # Retourne None si aucun encodage ne fonctionne

# Param√®tres de connexion MySQL (√† adapter)

def test_sql_connection(pdfile,temp_table)->any:
    TABLE=temp_table
    """Teste la connexion √† la base de donn√©es."""
    try:
        mysql_hook = MySqlHook(mysql_conn_id='mysql_conn')
        conn = mysql_hook.get_conn()
        cursor = conn.cursor()

        # V√©rification des bases de donn√©es disponibles
        cursor.execute("SHOW DATABASES;")
        print(cursor.fetchall())

        # Conversion des dates si pr√©sentes
        if "accounting_date" in pdfile.columns:
            pdfile["accounting_date"] = pd.to_datetime(pdfile["accounting_date"], format="%d/%m/%Y").dt.strftime("%Y-%m-%d")

        if "created_at" in pdfile.columns:
            pdfile["created_at"] = pd.to_datetime(pdfile["created_at"], format="%d/%m/%Y %H:%M").dt.strftime("%Y-%m-%d %H:%M")

        # V√©rification et conversion de quantity
        if "quantity" in pdfile.columns:
            pdfile["quantity"] = pdfile["quantity"].astype(str).str.replace(",", ".").astype(float)

        if "remaining_quantity" in pdfile.columns:
            pdfile["remaining_quantity"] = pdfile["remaining_quantity"].astype(str).str.replace(",", ".").astype(float)

        if "quantity_reserved" in pdfile.columns:
            pdfile["quantity_reserved"] = pdfile["quantity_reserved"].astype(str).str.replace(",", ".").astype(float)

        if "lettering_writing" in pdfile.columns:
            pdfile["lettering_writing"] = pdfile["lettering_writing"].astype(str).str.replace(",", ".").astype(float)

        if "sales_amount_actual" in pdfile.columns:
            pdfile["sales_amount_actual"] = pdfile["sales_amount_actual"].astype(str).str.replace(",", ".").astype(float)

        if "total_cost_actual" in pdfile.columns:
            pdfile["total_cost_actual"] = pdfile["total_cost_actual"].astype(str).str.replace(",", ".").astype(float)

        if "total_cost_not_included" in pdfile.columns:
            pdfile["total_cost_not_included"] = pdfile["total_cost_not_included"].astype(str).str.replace(",", ".").astype(float)

        if "kor_by_reception" in pdfile.columns:
            pdfile["kor_by_reception"] = pdfile["kor_by_reception"].astype(str).str.replace(",", ".").astype(float)

        if "kor_input" in pdfile.columns:
            pdfile["kor_input"] = pdfile["kor_input"].astype(str).str.replace(",", ".").astype(float)

        # Remplacement des NaN par ''
        pdfile = pdfile.where(pdfile.notna(), '')

        print("‚ö†Ô∏è Donn√©es avant insertion :", pdfile.dtypes)
        print("Aper√ßu des donn√©es :", pdfile.head())

        # Suppression des anciennes donn√©es
        delete_query = f"DELETE FROM {TABLE};"
        cursor.execute(delete_query)
        conn.commit()
        print("‚ö†Ô∏è Donn√©es supprim√©es avant insertion.")

        # Conversion en tuples pour l'insertion
        rows_to_insert = [tuple(row) for row in pdfile.itertuples(index=False)]
        #print('üìå Donn√©es √† ins√©rer :', rows_to_insert)

        # Insertion des donn√©es
        mysql_hook.insert_rows(table=TABLE, rows=rows_to_insert)
        print("‚úÖ Insertion r√©ussie !")

    except Exception as e:
        error_msg = f"‚ùå Erreur de connexion : {e}"
        print(error_msg)
        log_message(LOG_DIR, ERROR_FILENAME, error_msg)


def check_file_reliability_from_pandas(pdf:pd,expected_columns:any,column_types:any)->bool:
    result_check=True
    try:
        # V√©rifier si toutes les colonnes attendues sont pr√©sentes
        missing_columns = [col for col in expected_columns if col not in pdf.columns]
        
        if missing_columns:
            print(f"Taille de EXPECTED_COLUMNS: {len(EXPECTED_COLUMNS)}")
            print(f"Contenu de EXPECTED_COLUMNS: {EXPECTED_COLUMNS}")

            #Ecrire dans le log
            raise ValueError(f"Colonnes manquantes: {', '.join(missing_columns)}")
        # V√©rifier les types de donn√©es des colonnes
        for column, expected_type in column_types.items():
            if column in pdf.columns:
                for idx, value in enumerate(pdf[column]):
                    try:
                        # V√©rification Date
                        if expected_type == date:
                            value = pd.to_datetime(value, format="%d/%m/%Y").date()  

                        # V√©rification DateTime
                        elif expected_type == datetime:
                            value = pd.to_datetime(value, format="%d/%m/%Y %H:%M")  

                        # V√©rification Integer
                        elif expected_type == int:
                            try:
                                value = int(value)
                            except ValueError:
                                error_msg = f"‚ùå Erreur Ligne {idx} : {column} - {value} -> Type attendu : {expected_type.__name__}"
                                print(error_msg)
                                log_message(LOG_DIR, ERROR_FILENAME, error_msg)
                                return False

                        # V√©rification Float
                        elif expected_type == float:
                            if isinstance(value, str):  
                                value = value.replace(",", ".")  
                            value = float(value)  # Convertir en float
                            success_msg = f"‚úÖ Ligne {idx} : {column} - {value} -> Type correct (float)"

                        # V√©rification String
                        elif expected_type == str:
                            if not isinstance(value, str) and not pd.isna(value) and value != "":
                                error_msg = f"‚ùå Erreur Ligne {idx} : {column} - {value} -> Type attendu : {expected_type.__name__}"
                                print(error_msg)
                                log_message(LOG_DIR, ERROR_FILENAME, error_msg)
                                return False

                        else:
                            error_msg = f"‚ùå Erreur Ligne {idx} : {column} - {value} -> Type inconnu ({expected_type})"
                            print(error_msg)
                            log_message(LOG_DIR, ERROR_FILENAME, error_msg)
                            result_check = False

                    except (ValueError, TypeError):
                        result_check = False
                        error_msg = f"‚ùå Erreur Ligne {idx} : {column} - {value} -> Type attendu : {expected_type.__name__}"
                        print(error_msg)
                        log_message(LOG_DIR, ERROR_FILENAME, error_msg)

                error_msg=("‚úÖ Le fichier respecte les colonnes attendues et les types de donn√©es.",result_check)
                row_count = len(pdf)
                print('‚úÖLignes ',error_msg,row_count)
                log_message(LOG_DIR,"statut_date_execution.txt", error_msg)
                return result_check

        error_msg=("‚úÖ Le fichier respecte les colonnes attendues et les types de donn√©es.",result_check)
        row_count = len(pdf)
        print('‚úÖLignes ',error_msg,row_count)
        log_message(LOG_DIR,"statut_date_execution.txt", error_msg)
        return result_check

    except Exception as e:
        error_msg=(f"‚ùå Erreur de fiabilit√© : {str(e)}")
        print(error_msg)
        log_message(LOG_DIR,ERROR_FILENAME, error_msg)
        return False


# D√©finir la t√¢che de v√©rification de fiabilit√© du fichier
def verify_file_reliability(directory_path,allow_types,encodings,expected_columns,column_types,renamed_columns):
    print('camarche22',directory_path,allow_types,encodings,column_types,renamed_columns)
    files = [f for f in os.listdir(directory_path) if os.path.splitext(f)[1] in allow_types]
    for file in files:
        file_path = os.path.join(directory_path, file)
        pdFile= read_file(file,file_path,encodings,expected_columns,renamed_columns)
        if pdFile is not None:
            # ‚úÖ Remplacer `NaN` par `None`
            if check_file_reliability_from_pandas(pdFile,expected_columns,column_types):
                print('merci')
                cursor=test_sql_connection(pdFile,"temp_ecc")
                if not cursor:
                    error_msg='connexion sql echoue'
                    print(error_msg)
                    destination_out_path=f"{DIRECTORY_PATH}/{OUT_DIR}"
                    log_message(LOG_DIR,ERROR_FILENAME,error_msg)
                    move_file_to_out(file_path,destination_out_path,False)
                else:
                    destination_out_path=f"{DIRECTORY_PATH}/{OUT_DIR}"
                    print('connexion sql reussie',cursor)
                    move_file_to_out(file_path,destination_out_path)
                    return 'import_temp_ecc_to_ecc'
            else:
                print('Merde')
                error_msg='Erreur sur la retour fichier pandas'
                log_message(LOG_DIR,ERROR_FILENAME, error_msg)
                return 'end'
        else:
            #Ecrire dans le log
            error_msg=f'{file_path} non trait√© Boris :{pdFile}'
            print(file_path,file,error_msg)
            #log_message(ERROR_FILENAME, error_msg)
            log_message(LOG_DIR,ERROR_FILENAME, error_msg)

            return 




default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 2, 7),
    'retry_delay': timedelta(seconds=30),
    'retries': 1,
    'retry_delay': timedelta(days=1),

}

with DAG("IMPORTATION_ECRITURE_COMPTABLES_ARTICLE_dag", default_args=default_args, 
            tags=['boris', 'bahi'],
            #schedule_interval=None
            schedule=timedelta(seconds=60),  # Nouveau param√®tre
            start_date=datetime(2025, 2, 10),  # Date de d√©but du DAG
            catchup=False,  # Emp√™che l'ex√©cution r√©troactive des t√¢ches
            ) as dag:

    # Premi√®re t√¢che : v√©rifier si le r√©pertoire existe
    check_directory_task = BranchPythonOperator(
        task_id="check_directory",
        python_callable=check_directory,
        op_kwargs={'directory_path':DIRECTORY_PATH},  # Remplacer par votre chemin
    )

    # Deuxi√®me t√¢che : v√©rifier l'existence d'un type de fichier sp√©cifique
    check_file_task = BranchPythonOperator(
        task_id="check_file_in_directory",
        python_callable=check_file_in_directory,
        op_kwargs={'directory_path':f'{DIRECTORY_PATH}/{IN_DIR}','allow_types':ALLOWED_TYPES}

    )

     # Deuxi√®me t√¢che : v√©rifier l'existence d'un type de fichier sp√©cifique
   
    """
   # T√¢che 3: Lire chaque fichier si trouv√©
    read_files_task = BranchPythonOperator(
        task_id="read_files",
        python_callable=read_files
    )"""

    verify_file_reliability_task = BranchPythonOperator(
    task_id='verify_file_reliability',
    python_callable=verify_file_reliability,
    op_kwargs={'directory_path':f'{DIRECTORY_PATH}/{IN_DIR}','allow_types':ALLOWED_TYPES,
               'encodings':ENCODINGS,'expected_columns':EXPECTED_COLUMNS,
               'column_types':COLUMN_TYPES,'renamed_columns':RENAMED_COLUMNS}
)
    
    import_temp_ecc_to_ecc_task = BranchPythonOperator(
        task_id="import_temp_ecc_to_ecc",
        python_callable=import_temp_ecc_to_ecc,
    )

    # T√¢che de fin si aucun fichier valide ou r√©pertoire inexistant
    end_task = EmptyOperator(task_id="end")

   # D√©finition des d√©pendances
    #check_directory_task >> check_file_task >> read_files_task >> verify_file_reliability_task >> end_task
    check_directory_task >> check_file_task >> verify_file_reliability_task >> import_temp_ecc_to_ecc_task >> end_task

    # Si le r√©pertoire n'existe pas, on termine directement
    check_directory_task >> end_task
    # Si aucun fichier n'est trouv√©, on termine directement
    check_file_task >> end_task
    
    #read_files_task >> end_task

    # Si la v√©rification des fichiers √©choue, on termine directement
    verify_file_reliability_task >> end_task
     # Si la v√©rification des fichiers √©choue, on termine directement
    import_temp_ecc_to_ecc_task >> end_task